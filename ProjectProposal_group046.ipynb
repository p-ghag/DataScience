{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Sharon Chen\n",
    "- Pamela Ghag\n",
    "- Yuzi Chu\n",
    "- Cheng Chang\n",
    "- Stanley Hahm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there significant differences among courses under the Humanities, Social Sciences, and STEM departments at UCSD in terms of the correlation between course difficulty and teachers’ ratings on student-feedback platforms (e.g. CAPES, RateMyProfessor)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our interests in this question stem from the shared experience of using apps like Rate My Professor and Capes, to get a basic understanding of certain classes and the teaching style of a professor. These services allow students to give feedback and ratings to their education. Meanwhile, other students who view these comments and feedback could get helpful advice on deciding whether to take a class or not. However, even authenticated students’ feedback does not always reflect the actual teaching quality of certain professors. We noticed that students give ratings of professors highly based on the difficulty level of the courses, especially in STEM-focused courses.\n",
    "\n",
    "This topic has been previously discussed by researchers. For example, Justin Esarey published a paper about Unbiased, reliable, and valid student evaluations can still be unfair to debate potential flaws in imperfect measures and unfair evaluations (1). In Colleen Flaherty’s blog on Inside Higher Ed, Study: Student evaluations of teaching are deeply flawed, she indicates that biased evaluations might cause the faculty to punish professors who are marginalized to teach unpopular courses (2).\n",
    "\n",
    "Therefore, understanding what factors impact the rating result and the possible biases are essential to both educators and students. With this in mind, we would like to investigate the question: Are there significant differences among courses under the Humanities, Social Sciences, and STEM departments at UCSD in terms of the correlation between course difficulty and teachers’ ratings on student-feedback platforms (e.g. CAPES, Rate My Professor)?\n",
    "\n",
    "To get at a potential answer, we plan to compare the results from humanities courses, Social Science, and STEM-focused courses on different student rating platforms such as RateMyProfessor and Capes. We assume there is a close relationship between the difficulty of the course and the rating results received by the teaching staff, which we aim to reveal with data analysis. We want to observe whether the correlation between course difficulty and teachers’ ratings holds constant across the Social Sciences, STEM, and Humanities departments, or whether the correlation is stronger/weaker for one department over another.\n",
    "\n",
    "As a first step, we hand-picked some field of studies offered at UCSD and listed them into three categories:\n",
    "- __STEM__: Biology, Chemistry, Computer Science, Data Science, Mathematics, Physics, All Engineering;\n",
    "- __Social Science__: Political Science, Sociology, Psychology, Cognitive Science, Economics (3);\n",
    "- __Humanities__: Fine Arts, Literature, Language Studies, Theatre, Film, English, Philosophy, Visual Arts, History.\n",
    "\n",
    "\n",
    "\n",
    "#### References (include links):\n",
    "1. https://www.tandfonline.com/eprint/IMZDUUHTHCEDD4Q9VZYB/full?target=10.1080%2F02602938.2020.1724875&\n",
    "2. https://www.insidehighered.com/news/2020/02/27/study-student-evaluations-teaching-are-deeply-flawed\n",
    "3. https://socialsciences.ucsd.edu/programs/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that the teaching staff rating provided by students may be more correlated with the difficulty of the course if it is under the STEM category when compared to courses in humanities or social science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal datasets would be Rate My Professor’s dataset and cape’s dataset.\n",
    "\n",
    "#### RateMyProfessor:\n",
    "- Variables: Overall Quality, Would take again, Level of Difficulty, Name of Course, Text of Review\n",
    "- Number of Observations: All specified UCSD professors\n",
    "- Who/what: FA2014 - FA2019, specific departments listed above in the background section\n",
    "- How: https://pypi.org/project/RateMyProfessorPyAPI/, https://data.mendeley.com/datasets/fvtfjyvw7d/2 \n",
    "\n",
    "#### Cape:\n",
    "- Variables: Recommend Instructor, Study Hrs/wk, Avg Grade Expected, Avg Grade Received, Name of Course\n",
    "- Number of Observations: number of professors specified \n",
    "- Who/what: FA2015-FA2019, specific departments listed below\n",
    "- How: https://gist.github.com/jjangsangy/ef0d9b534c5f4ab58422\n",
    "\n",
    "After we gather our data, we will be branching the data between departments (Humanities, Social Science, and STEM). From then on, we will timestamp all the data for each site: RateMyProfessor by the date the rating was posted & Capes for the time of the quarter evaluation.\n",
    "\n",
    "Then we would check for any bias. It is listed in the Ethics & Privacy, but we will be cleaning any data that has professors doing both upper & lower division classes along w/ any reviews (on RateMyProfessors) that may have been emotional or biased.\n",
    "\n",
    "Then we can evaluate the variables listed. For the variables, each site has similar data that cross-reference with each other.\n",
    "- “Would take again” = “Recommend Instructor”\n",
    "- “Level of Difficulty” can correlate with “Study Hrs/wk”\n",
    "- “Name of Course” = “Name of Course”\n",
    "- (This is testing our hypothesis) “Quality” can potentially correlate w/ “Avg Grade Received”. This will show if the harder the class (hence a lower grade) can have a correlation w/ the “Quality” rating "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will be using is available in the public domain, we used RateMyProfessor (review website) to collect our data. We also pulled data from CAPE (Course And Professor Evaluations), which is a representation of University of California, San Diego students. In regards to privacy concerns towards the teachers that are evaluated in our data we did not use their real identities also to ensure specific courses remained anonymous we assigned them each an identification code.\n",
    "\n",
    "To ensure we don’t have a bias issue within our data we have taken into account the global pandemic, and acknowledge that teaching styles have changed dramatically as the majority of classes are currently taken online. Therefore, we decided to only examine data that was available before the pandemic (anything before Fall 2019).\n",
    "\n",
    "Another method we proceeded to implement to ensure we have minimal bias was the number of courses we looked at. For our data we decided to use 50 upper and 50 lower-division courses, therefore we didn’t have an issue with the data we have collected. However, we did ensure if in any circumstance we have come across a teacher teaching in an upper and lower division course that we don’t count them multiple times in our data.\n",
    "\n",
    "We also had to take into account the disparity of genders amongst the teaching staff. If possible, we would like to balance the number of teaching staff with different self-identified genders.\n",
    "\n",
    "We have chosen to use social science courses as a point of comparison between STEM-focused and humanitarian courses because they are interdisciplinary courses, and these courses are intertwined between the two departments we are analyzing.\n",
    "\n",
    "We have also considered that a review by a student could be dependent on their personal issues and not primarily due to the teacher conducting the course or even the course difficulty.\n",
    "\n",
    "We understand all these factors can cause bias in our analysis and will take these into account in our final write-up. Although we planned to use a wide range of data, we have tried to ensure we have minimal bias and privacy issues. We believe them to be transparent as we could acquire. Lastly, we don’t anticipate our findings to lead to any negative outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All group members must attend weekly meetings on Tuesdays, 1 pm PST; in cases of unavailability, they must notify the group at least 24 hours in advance about their absence.\n",
    "- All group members must complete their delegated tasks by the weekly meetings (if applicable), or at least be prepared to give a status update.\n",
    "- All group members must maintain an open line of communication (via Discord group chat or email).\n",
    "\n",
    "- Some guidelines for GitHub repository management:\n",
    "    - For new additions or modifications of existing codes and writeups in the GitHub repository, please create a new branch and make changes there.\n",
    "    - After the changes, please set up a pull request and invite at least on group members to review before merging.\n",
    "    - Please utilize the GitHub __Issue__ system to track individual workload and group deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM | Read & Think about COGS 108 expectations; brainstorm topics/questions  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data (Ant Man); EDA (Hulk) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin Analysis (Iron Man; Thor) | Discuss/edit Analysis; Complete project check-in |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Wasp)| Discuss/edit full project |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project & Group Project Surveys |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
